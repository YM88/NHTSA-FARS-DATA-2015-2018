{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "Dataset is acquired from NHTSA [ftp](ftp://ftp.nhtsa.dot.gov/FARS/)\n",
    "\n",
    "Most recent manuals and publications can be accessed via the [CrashStats](https://crashstats.nhtsa.dot.gov/#/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load the dataset\n",
    "\n",
    "All CSV.zip files should be in the 'data' folder. There's no need for extraction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:34:32.086565Z",
     "start_time": "2019-11-04T19:34:32.057804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading libraries\n",
    "# pd.reset_option('max_rows')\n",
    "# pd.reset_option('max_columns')\n",
    "#tweaks\n",
    "import pandas as pd\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#sets up pandas table display\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "pd.set_option('precision', 5)\n",
    "#ETL and EDA\n",
    "import zipfile\n",
    "import time\n",
    "import numpy as np # imports a fast numerical programming library\n",
    "import scipy as sp #imports stats functions, amongst other things\n",
    "#Visualization, MPL, SNS, PY\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl # this actually imports matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm #allows us easy access to colormaps\n",
    "import matplotlib.pyplot as plt #sets up plotting under plt\n",
    "from matplotlib import rcParams # special matplotlib argument for improved plots\n",
    "\n",
    "import seaborn as sns #sets up styles and gives us more plotting options\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"poster\")\n",
    "# Standard plotly imports\n",
    "import plotly_express as px\n",
    "import chart_studio.plotly as py\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# Using plotly / cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "##Machine Learning\n",
    "#statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "#sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import RFE #recursive feature selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Accidents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:33:02.741886Z",
     "start_time": "2019-11-04T19:33:02.518270Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing df\n",
    "with zipfile.ZipFile('data/FARS2018NationalCSV.zip') as zip:\n",
    "    with zip.open('ACCIDENT.csv') as my_csv:\n",
    "        accidents_18 = pd.read_csv(my_csv) \n",
    "\n",
    "# converting col names to lowercase\n",
    "accidents_18.columns = accidents_18.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new seconds column to deal with the 99 values or unknown reported for 245 rows. Method is to use the value 30 for the seconds to keep track of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:50:01.069356Z",
     "start_time": "2019-11-04T19:50:01.033918Z"
    }
   },
   "outputs": [],
   "source": [
    "accidents_18['second'] = 0\n",
    "accidents_18['second'][(accidents_18.hour == 99) |(accidents_18.minute == 99)] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:50:01.483970Z",
     "start_time": "2019-11-04T19:50:01.463659Z"
    }
   },
   "outputs": [],
   "source": [
    "accidents_18['hour'][accidents_18.hour == 99] = 0\n",
    "accidents_18['minute'][accidents_18.minute == 99] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:50:02.682546Z",
     "start_time": "2019-11-04T19:50:02.628092Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating date time object by the 5 columns of df\n",
    "accidents_18['date'] = pd.to_datetime(accidents_18[['day', 'month', 'year', 'hour', 'minute', 'second']])\n",
    "accidents_18 = accidents_18.sort_values('date').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value dictionaries  for categorical values of routes, states, weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:50:07.377125Z",
     "start_time": "2019-11-04T19:50:07.297087Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating states/weather/route dictionary, values recorded from: \n",
    "\n",
    "\n",
    "\n",
    "routes = {1: 'Interstate', 2: 'U.S. Highway',3: 'State Highway',\n",
    "          4: 'County Road',5: 'Local Street – Township',\n",
    "          6: 'Local Street – Municipality',7: 'Local Street – Frontage Road',\n",
    "          8: 'Other',9: 'Unknown'}\n",
    "\n",
    "\n",
    "states = {1: 'AL', 2: 'AK', 4: 'AZ', 5: 'AR',\n",
    "          6: 'CA', 8: 'CO', 9: 'CT', 10: 'DE',\n",
    "          11: 'DC', 12: 'FL', 13: 'GA', 15: 'HI',\n",
    "          16: 'ID', 17: 'IL', 18: 'IN', 19: 'IA',\n",
    "          20: 'KS', 21: 'KY', 22: 'LA', 23: 'ME',\n",
    "          24: 'MD', 25: 'MA', 26: 'MI', 27: 'MN',\n",
    "          28: 'MS', 29: 'MO', 30: 'MT', 31: 'NE',\n",
    "          32: 'NV', 33: 'NH', 34: 'NJ', 35: 'NM',\n",
    "          36: 'NY', 37: 'NC', 38: 'ND', 39: 'OH',\n",
    "          40: 'OK', 41: 'OR', 42: 'PN', 43: 'PR',\n",
    "          44: 'RI', 45: 'SC', 46: 'SD', 47: 'TN',\n",
    "          48: 'TX', 49: 'UT', 50: 'VT', 51: 'VA',\n",
    "          52: 'VI', 53: 'WA', 54: 'WV', 55: 'WI', 56: 'WY'}\n",
    "\n",
    "weather = {0: 'No Additional Atmospheric Conditions', 1: 'Clear',\n",
    "           2: 'Rain', 3: 'Sleet, Hail',\n",
    "           4: 'Snow', 5: 'Fog, Smog, Smoke', 6: 'Severe Crosswinds',\n",
    "           7: 'Blowing Sand, Soil, Dirt',\n",
    "           8: 'Other', 10: 'Cloudy', 11: 'Blowing Snow',\n",
    "           12: 'Freezing Rain or Drizzle',\n",
    "           98: 'Not Reported', 99: 'Unknown'}\n",
    "\n",
    "# replacing values in df\n",
    "accidents_18['route'] = accidents_18['route'].map(routes)\n",
    "accidents_18['state'] = accidents_18['state'].map(states)\n",
    "accidents_18['weather'] = accidents_18['weather'].map(weather)\n",
    "accidents_18['weather1'] = accidents_18['weather1'].map(weather)\n",
    "accidents_18['weather2'] = accidents_18['weather2'].map(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:37:40.096157Z",
     "start_time": "2019-11-04T19:37:40.045195Z"
    }
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/FARS2018NationalCSV.zip') as zip:\n",
    "    with zip.open('VIOLATN.csv') as my_csv:\n",
    "        violations_18 = pd.read_csv(my_csv) \n",
    "\n",
    "# converting col names to lowercase\n",
    "violations_18.columns = violations_18.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating groups for violations\n",
    "Violations are grouped into three categories of reckless driving, impaired driver, and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](info/mviolatns_1.png)\n",
    "\n",
    "![](info/mviolatns_2.png)\n",
    "\n",
    "![](info/mviolatns_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T19:37:41.863882Z",
     "start_time": "2019-11-04T19:37:41.165663Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating a dictionary for grouping the violations into three groups\n",
    "reckless = {j: \"Reckless\" for j in range(1, 11)}\n",
    "impaired = {j: \"Impaired\" for j in range(11, 20)}\n",
    "other = {j: \"Other\" for j in range(20, 100)}\n",
    "violations = {0: \"Other\", **reckless, **impaired, **other}\n",
    "violations_18['mviolatn'] = violations_18.mviolatn.map(violations)\n",
    "violations_18 = pd.DataFrame(violations_18.groupby(['st_case'])['mviolatn'].agg(set)).reset_index()\n",
    "violation_groups = {\n",
    "\"{'Other'}\":\"Other\",\n",
    "\"{'Other', 'Reckless'}\": \"Reckless\",\n",
    "\"{'Reckless'}\": \"Reckless\",\n",
    "\"{'Impaired', 'Other', 'Reckless'}\": \"Impaired, Reckless\",\n",
    "\"{'Impaired', 'Other'}\": \"Impaired\",\n",
    "\"{'Impaired'}\": \"Impaired\",\n",
    "\"{'Impaired', 'Reckless'}\": \"Impaired, Reckless\"}\n",
    "violations_18.mviolatn = violations_18.mviolatn.astype(str).map(violation_groups)\n",
    "\n",
    "#merging violations groups and accidents per st_case\n",
    "\n",
    "accidents_18 = accidents_18.merge(violations_18, on='st_case')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Null values:\n",
    "\n",
    "Dataset has many na values in different lengths of 9s and 8s: \n",
    "\n",
    "Depending on the column this length changes: as an example 99 and 98 are unknown and not reported weather values, yet there's also a 99 county code value that is valid for the respective county.\n",
    "\n",
    "As a caution this values were not used during the read_csv method and will be dealt column by column to avoid introducing false NAs in the dataset.\n",
    "\n",
    "na_values = [88, 98, 99, 999, 99997, 8888, 9999, 99998, 99999,\n",
    "             999999, 9999999, 99999999, 999999999, 9999999999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering all the columns except the county and the newly created date column\n",
    "#replacing the values in na_values with nans\n",
    "na_values = [88, 98, 99, 999, 99997, 8888, 9999, 99998, 99999, 999999, 9999999, 99999999, 999999999, 9999999999]\n",
    "accidents_18[accidents_18.columns.difference(['date', 'county'])].replace(na_values, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_18.fatals.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_18['text'] = accidents_18.state +' - ' + accidents_18.date.dt.month_name(\n",
    ") + ' ' + accidents_18.date.dt.day.astype(str) + ', ' + accidents_18.fatals.astype(str) + ' Killed'\n",
    "\n",
    "fig = go.Figure(data=go.Scattergeo(\n",
    "\n",
    "    lon=accidents_18.longitud,\n",
    "    lat=accidents_18.latitude,\n",
    "    text=accidents_18.text,\n",
    "    marker_size=accidents_18.fatals ** 0.5 * 5,\n",
    "    marker_opacity=0.75,\n",
    "    marker_color='rgb(200, 0, 0)'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Traffic Fatalities by Location in U.S. (2018)<br>''<sub>Collision Date and Deaths</sub>',\n",
    "    geo=dict(\n",
    "        scope='usa',\n",
    "        projection_type='albers usa',\n",
    "        showland=True,\n",
    "        landcolor=\"rgb(250, 250, 250)\",\n",
    "        subunitcolor=\"rgb(217, 217, 217)\",\n",
    "        countrycolor=\"rgb(217, 217, 217)\",\n",
    "        countrywidth=0.5,\n",
    "        subunitwidth=0.5))\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#make title bigger\n",
    "#make size bigger\n",
    "#change color?\n",
    "#animation per year when other data frames added\n",
    "#or animation frame per how many people died?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_18[accidents_18.date.dt.day.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average fatals group by state and their census population in 2018\n",
    "\n",
    "state_fatals_18_sum = accidents_18.groupby(['state']).agg({'fatals':sum}).sort_values('fatals', ascending = False).reset_index()\n",
    "\n",
    "fig = px.bar(state_fatals_18_sum, x ='state', y = 'fatals',\n",
    "             labels={'state':'State', 'fatals':'Fatalities'},\n",
    "             title = 'Traffic Fatalities by State in U.S. 2018',\n",
    "             template = 'plotly_white', color_discrete_sequence=px.colors.qualitative.Set1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#animation after adding 4 years per year?\n",
    "#divided per propulation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fatalities per hour?\n",
    "#fatalities per day?\n",
    "#fatalities per month?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual to Nearest Tenth Mile\n",
    "# (Assume decimal, e.g., 12345 = 1234.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatalities by VMT\n",
    "Fatalities by VMT is a unit for assessing road traffic fatalities. This metric is computed by dividing the fatalities by the estimated VMT.\n",
    "\n",
    "Usually, transport risk is computed by reference to the distance traveled by people, while for road traffic risk, only vehicle traveled distance is usually taken into account.[6]\n",
    "\n",
    "In the United-States, the unit is used as an aggregate in yearly federal publications, while its usage is more sporadic in other countries. For instance, it appears to compare different kind of roads in some publications as it had been computed on a fiver years period between 1995 and 2000.[7]\n",
    "\n",
    "In the United States, it is computed per 100 million miles traveled, while internationally it is computed in 100 million or 1 billion kilometers traveled.\n",
    "\n",
    "According to the Minnesota Department of Public Safety, Office of Traffic Safety\n",
    "\n",
    "Volume of traffic, or vehicle miles traveled (VMT), is a predictor of crash incidence. All other things being equal, as VMT increases, so will traffic crashes. The relationship may not be simple, however; after a point, increasing congestion leads to reduced speeds, hanging the proportion of crashes that occur at different severity levels.[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation of VMT, weather temperature, new driver?\n",
    "accidents_18_corr = accidents_18.corr()\n",
    "accidents_18_corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
